# HANDLE JAPANESE TEXT
import MeCab
tokenizer = MeCab.Tagger()
use_pos_list = {{ task.use_pos_list }}
use_word_stemming = {{ task.use_word_stemming }}
def tokenize(text, use_pos_list, use_word_stemming, tokenizer):
    node = tokenizer.parseToNode(text)
    terms = []
    while node:
        features = node.feature.split(",")
        pos = features[0]
        if pos != "BOS/EOS":
            if use_word_stemming:
                term = features[6]
                if (pos == "名詞") & (features[1] == "数"):
                    term = node.surface
            else:
                term = node.surface
            if use_pos_list:
                if pos in use_pos_list:
                    terms.append(term)
            else:
                terms.append(term)
        node = node.next
    return " ".join(terms)
cols_japanese_text = {{ cols_japanese_text}}
for col in cols_japanese_text:
{% if training %}
    __train_dataset[col] = __train_dataset[col].fillna("").apply(lambda x: tokenize(x, use_pos_list, use_word_stemming, tokenizer))
{% endif %}
{% if test %}
    __test_dataset[col] = __test_dataset[col].fillna("").apply(lambda x: tokenize(x, use_pos_list, use_word_stemming, tokenizer))
{% endif %}


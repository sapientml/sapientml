{% if target2string %}
__target_test = __target_test.astype(str)
{% endif %}
{% if pipeline.adaptation_metric == macros.Metric.AUC.value %}
from sklearn.metrics import roc_auc_score
{% if pipeline.task.is_multiclass == True %}
__auc = roc_auc_score(__target_test.values.ravel(), __y_pred, multi_class="ovr")
{% else %}
__auc = roc_auc_score(__target_test, __y_pred)
{% endif %}
print('RESULT: AUC Score: ' + str(__auc))
{% elif (pipeline.adaptation_metric == macros.Metric.Accuracy.value) and (not pipeline.is_multi_class_multi_targets) %}
from sklearn.metrics import accuracy_score

__accuracy = accuracy_score(__target_test, __y_pred)
print('RESULT: Accuracy: ' + str(__accuracy))
{% elif (pipeline.adaptation_metric == macros.Metric.Accuracy.value) and (pipeline.is_multi_class_multi_targets) %}
from sklearn.metrics import accuracy_score

__accs = []
for i, col in enumerate(__target_test.columns):
    __one_acc = accuracy_score(__target_test[col], __y_pred[:, i:i+1])
    __accs.append(__one_acc)
print(f"RESULT: Accuracy : {str(sum(__accs)/len(__accs))}")
{% elif pipeline.adaptation_metric == macros.Metric.F1.value %}
from sklearn import metrics

__f1 = metrics.f1_score(__target_test, __y_pred, average='macro')
print('RESULT: F1 Score: ' + str(__f1))
{% elif pipeline.adaptation_metric == macros.Metric.R2.value %}
from sklearn import metrics

__r2 = metrics.r2_score(__target_test, __y_pred)
print('RESULT: R2 Score:', str(__r2))
{% elif pipeline.adaptation_metric == macros.Metric.RMSE.value %}
from sklearn.metrics import mean_squared_error

__rmse = mean_squared_error(__target_test, __y_pred, squared=False)
print('RESULT: RMSE:', str(__rmse))
{% elif pipeline.adaptation_metric == macros.Metric.RMSLE.value %}
import numpy as np
from sklearn.metrics import mean_squared_log_error

__target_test = np.clip(__target_test, 0, None)
__y_pred = np.clip(__y_pred, 0, None)
__rmsle = np.sqrt(mean_squared_log_error(__target_test, __y_pred))
print('RESULT: RMSLE:', str(__rmsle))
{% elif pipeline.adaptation_metric == macros.Metric.Gini.value %}
from sklearn.metrics import roc_auc_score
{% if pipeline.task.is_multiclass == True %}
__gini = 2 * roc_auc_score(__target_test.values.ravel(), __y_pred, multi_class="ovr") - 1
{% else %}
__gini = 2 * roc_auc_score(__target_test, __y_pred) - 1
{% endif %}
print('RESULT: Gini: ' + str(__gini))
{% elif pipeline.adaptation_metric == macros.Metric.MAE.value %}
from sklearn.metrics import mean_absolute_error

__mae = mean_absolute_error(__target_test, __y_pred)
print('RESULT: MAE:', str(__mae))
{% elif pipeline.adaptation_metric == macros.Metric.LogLoss.value %}
from sklearn.metrics import log_loss

__log_loss = log_loss(__target_test, __y_pred)
print('RESULT: Log Loss:', str(__log_loss))
{% elif pipeline.adaptation_metric == macros.Metric.ROC_AUC.value %}
from sklearn.metrics import roc_auc_score
{% if pipeline.task.is_multiclass == True %}
__roc_auc = roc_auc_score(__target_test.values.ravel(), __y_pred, multi_class="ovr")
{% else %}
__roc_auc = roc_auc_score(__target_test, __y_pred)
{% endif %}
print('RESULT: ROC AUC:', str(__roc_auc))
{% elif pipeline.adaptation_metric == macros.Metric.MCC.value %}
from sklearn.metrics import matthews_corrcoef

__mcc = matthews_corrcoef(__target_test, __y_pred)
print('RESULT: MCC:', str(__mcc))
{% elif pipeline.adaptation_metric.startswith("MAP_") %}
{% set k = pipeline.adaptation_metric.split("_")[1] %}
def apk(actual, predicted, k):
    if len(predicted)>k:
        predicted = predicted[:k]

    score = 0.0
    num_hits = 0.0

    for i,p in enumerate(predicted):
        if p in actual and p not in predicted[:i]:
            num_hits += 1.0
            score += num_hits / (i+1.0)

    return score / min(len(actual), k)

def mapk(actual, predicted, k):
    """ Computes the mean average precision at k.

    Args:
        actual (list[list[str] or ndarray): A list of lists of elements that are to be predicted
        predicted (list[list[str] or ndarray): A list of lists of predicted elements
            (In each list, arrange in the order you predicted.)
        k (int): The maximum number of predicted elements

    Returns:
        double: The mean average precision at k over the input lists
    """
    return np.mean([apk(a,p,k) for a,p in zip(actual, predicted)])

__map_k = mapk(__target_test.to_numpy(), __y_pred, k={{ k }})
print('RESULT: MAP@K: ' + str(__map_k))
{% elif pipeline.adaptation_metric == macros.Metric.QWK.value %}
from sklearn.metrics import cohen_kappa_score

__qwk = cohen_kappa_score(__target_test, __y_pred, weights='quadratic')
print('RESULT: QWK:', str(__qwk))
{% elif pipeline.task_type == macros.TASK_REGRESSION.value %}
from sklearn import metrics

__r2 = metrics.r2_score(__target_test, __y_pred)
print('RESULT: R2 Score:', str(__r2))
{% else %}
from sklearn import metrics

__f1 = metrics.f1_score(__target_test, __y_pred, average='macro')
print('RESULT: F1 Score: ' + str(__f1))
{% endif %}
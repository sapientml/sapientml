#EVALUATION
if set(_TARGET_COLUMNS).issubset(__test_dataset.columns.tolist()):
{% if pipeline.adaptation_metric == macros.Metric.AUC.value %}
    from sklearn.metrics import roc_auc_score

    __y_pred = np.transpose([pred[:, 1] for pred in __y_pred])
    __auc = roc_auc_score(__target_test, __y_pred)
    print('RESULT: AUC Score: ' + str(__auc))
{% elif (pipeline.adaptation_metric == macros.Metric.Accuracy.value) and (not pipeline.is_multi_class_multi_targets) %}
    from sklearn.metrics import accuracy_score

    __accuracy = accuracy_score(__target_test, __y_pred)
    print('RESULT: Accuracy: ' + str(__accuracy))
{% elif (pipeline.adaptation_metric == macros.Metric.Accuracy.value) and (pipeline.is_multi_class_multi_targets) %}
    from sklearn.metrics import accuracy_score

    __accs = []
    for i, col in enumerate(__target_test.columns):
        __one_acc = accuracy_score(__target_test[col], __y_pred[:, i:i+1])
        __accs.append(__one_acc)
    print(f"RESULT: Accuracy : {str(sum(__accs)/len(__accs))}")
{% elif pipeline.adaptation_metric == macros.Metric.F1.value %}
    from sklearn import metrics

    __f1 = metrics.f1_score(__target_test, __y_pred, average='macro')
    print('RESULT: F1 Score: ' + str(__f1))
{% elif pipeline.adaptation_metric == macros.Metric.R2.value %}
    from sklearn import metrics

    __r2 = metrics.r2_score(__target_test, __y_pred)
    print('RESULT: R2 Score:', str(__r2))
{% elif pipeline.adaptation_metric == macros.Metric.RMSE.value %}
    from sklearn.metrics import mean_squared_error

    __rmse = mean_squared_error(__target_test, __y_pred, squared=False)
    print('RESULT: RMSE:', str(__rmse))
{% elif pipeline.adaptation_metric == macros.Metric.RMSLE.value %}
    import numpy as np
    from sklearn.metrics import mean_squared_log_error

    __target_test = np.clip(__target_test, 0, None)
    __y_pred = np.clip(__y_pred, 0, None)
    __rmsle = np.sqrt(mean_squared_log_error(__target_test, __y_pred))
    print('RESULT: RMSLE:', str(__rmsle))
{% elif pipeline.adaptation_metric == macros.Metric.Gini.value %}
    import numpy as np


    def gini(actual, pred, cmpcol = 0, sortcol = 1):
        assert( len(actual) == len(pred) )
        all = np.asarray(np.c_[ actual, pred, np.arange(len(actual)) ], dtype=np.float)
        all = all[ np.lexsort((all[:,2], -1*all[:,1])) ]
        totalLosses = all[:,0].sum()
        giniSum = all[:,0].cumsum().sum() / totalLosses

        giniSum -= (len(actual) + 1) / 2.
        return giniSum / len(actual)

    def gini_normalized(a, p):
        return gini(a, p) / gini(a, a)

    __gini = gini(__target_test, __y_pred)
    print('RESULT: Gini: ' + str(__gini))
{% elif pipeline.adaptation_metric == macros.Metric.MAE.value %}
    from sklearn.metrics import mean_absolute_error

    __mae = mean_absolute_error(__target_test, __y_pred)
    print('RESULT: MAE:', str(__mae))
{% elif pipeline.adaptation_metric == macros.Metric.LogLoss.value %}
    from sklearn.metrics import log_loss

    __log_loss = log_loss(__target_test, __y_pred)
    print('RESULT: Log Loss:', str(__log_loss))
{% elif pipeline.adaptation_metric == macros.Metric.ROC_AUC.value %}
    from sklearn.metrics import roc_auc_score

    __roc_auc = roc_auc_score(__target_test, __y_pred)
    print('RESULT: ROC AUC:', str(__roc_auc))
{% elif pipeline.adaptation_metric == macros.Metric.MCC.value %}
    from sklearn.metrics import matthews_corrcoef

    __mcc = matthews_corrcoef(__target_test, __y_pred)
    print('RESULT: MCC:', str(__mcc))
{% elif pipeline.adaptation_metric.startswith("MAP_") %}
{% set k = pipeline.adaptation_metric.split("_")[1] %}
    def apk(actual, predicted, k):
        if len(predicted)>k:
            predicted = predicted[:k]

        score = 0.0
        num_hits = 0.0

        for i,p in enumerate(predicted):
            if p in actual and p not in predicted[:i]:
                num_hits += 1.0
                score += num_hits / (i+1.0)

        return score / min(len(actual), k)

    def mapk(actual, predicted, k):
        """ Computes the mean average precision at k.

        Args:
            actual (list[list[str] or ndarray): A list of lists of elements that are to be predicted
            predicted (list[list[str] or ndarray): A list of lists of predicted elements
                (In each list, arrange in the order you predicted.)
            k (int): The maximum number of predicted elements

        Returns:
            double: The mean average precision at k over the input lists
        """
        return np.mean([apk(a,p,k) for a,p in zip(actual, predicted)])

    __map_k = mapk(__target_test.to_numpy(), __y_pred, k={{ k }})
    print('RESULT: MAP@K: ' + str(__map_k))
{% elif pipeline.adaptation_metric == macros.Metric.QWK.value %}
    from sklearn.metrics import cohen_kappa_score

    __qwk = cohen_kappa_score(__target_test, __y_pred, weights='quadratic')
    print('RESULT: QWK:', str(__qwk))
{% elif pipeline.task_type == macros.TASK_REGRESSION.value %}
    from sklearn import metrics

    __r2 = metrics.r2_score(__target_test, __y_pred)
    print('RESULT: R2 Score:', str(__r2))
{% else %}
    from sklearn import metrics

    __f1 = metrics.f1_score(__target_test, __y_pred, average='macro')
    print('RESULT: F1 Score: ' + str(__f1))
{% endif %}

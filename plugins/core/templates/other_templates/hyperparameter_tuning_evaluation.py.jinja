{% if target2string %}
        __target_test = __target_test.astype(str)
{% endif %}
{% if pipeline.adaptation_metric == macros.Metric.AUC.value %}
        from sklearn.metrics import roc_auc_score
        score = roc_auc_score(__target_test, __y_pred)
{% elif pipeline.adaptation_metric == macros.Metric.Accuracy.value %}
        from sklearn.metrics import accuracy_score
        score = accuracy_score(__target_test, __y_pred)
{% elif pipeline.adaptation_metric == macros.Metric.F1.value %}
        from sklearn import metrics
        score = metrics.f1_score(__target_test, __y_pred, average='macro')
{% elif pipeline.adaptation_metric == macros.Metric.R2.value %}
        from sklearn import metrics
        score = metrics.r2_score(__target_test, __y_pred)
{% elif pipeline.adaptation_metric == macros.Metric.RMSE.value %}
        from sklearn.metrics import mean_squared_error
        score = mean_squared_error(__target_test, __y_pred, squared=False)
{% elif pipeline.adaptation_metric == macros.Metric.RMSLE.value %}
        import numpy as np
        from sklearn.metrics import mean_squared_log_error
        __target_test = np.clip(__target_test, 0, None)
        __y_pred = np.clip(__y_pred, 0, None)
        score = np.sqrt(mean_squared_log_error(__target_test, __y_pred))
{% elif pipeline.adaptation_metric == macros.Metric.Gini.value %}
        import numpy as np
        def gini(actual, pred, cmpcol = 0, sortcol = 1):
            assert( len(actual) == len(pred) )
            all = np.asarray(np.c_[ actual, pred, np.arange(len(actual)) ], dtype=np.float)
            all = all[ np.lexsort((all[:,2], -1*all[:,1])) ]
            totalLosses = all[:,0].sum()
            giniSum = all[:,0].cumsum().sum() / totalLosses
        
            giniSum -= (len(actual) + 1) / 2.
            return giniSum / len(actual)
    
        def gini_normalized(a, p):
            return gini(a, p) / gini(a, a)
        
        score = gini(__target_test, __y_pred)
{% elif pipeline.adaptation_metric == macros.Metric.MAE.value %}
        from sklearn.metrics import mean_absolute_error
        score = mean_absolute_error(__target_test, __y_pred)
{% elif pipeline.adaptation_metric == macros.Metric.LogLoss.value %}
        from sklearn.metrics import log_loss
        score = log_loss(__target_test, __y_pred)
{% elif pipeline.adaptation_metric == macros.Metric.ROC_AUC.value %}
        from sklearn.metrics import roc_auc_score
        score = roc_auc_score(__target_test, __y_pred)
{% elif pipeline.adaptation_metric.startswith("MAP_") %}
{% set k = pipeline.adaptation_metric.split("_")[1] %}
        def apk(actual, predicted, k):
            if len(predicted)>k:
                predicted = predicted[:k]
        
            score = 0.0
            num_hits = 0.0
    
            for i,p in enumerate(predicted):
                if p in actual and p not in predicted[:i]:
                    num_hits += 1.0
                    score += num_hits / (i+1.0)
    
            return score / min(len(actual), k)
    
        def mapk(actual, predicted, k):
            """ Computes the mean average precision at k.

            Args:
                actual (list[list[str] or ndarray): A list of lists of elements that are to be predicted
                predicted (list[list[str] or ndarray): A list of lists of predicted elements
                    (In each list, arrange in the order you predicted.)
                k (int): The maximum number of predicted elements

            Returns:
                double: The mean average precision at k over the input lists
            """
            return np.mean([apk(a,p,k) for a,p in zip(actual, predicted)])
    
        score = mapk(__target_test.to_numpy(), __y_pred, k={{ k }})
{% elif pipeline.task.task_type == macros.TASK_REGRESSION %}
        from sklearn import metrics
        score = metrics.r2_score(__target_test, __y_pred)
{% else %}
        from sklearn import metrics
        score = metrics.f1_score(__target_test, __y_pred, average='macro')
{% endif %}